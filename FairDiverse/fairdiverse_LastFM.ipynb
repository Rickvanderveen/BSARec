{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import fairdiverse\n",
    "import yaml\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/github/school/master1/artifical-intelligence/recommender-systems/BSARec/FairDiverse/fairdiverse\n"
     ]
    }
   ],
   "source": [
    "cd fairdiverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p recommendation/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_results(model_name, dataset_name):\n",
    "    if dataset_name !=\"\":\n",
    "        today = date.today()\n",
    "        today_format = f\"{today.year}-{today.month}-{today.day}\"\n",
    "\n",
    "        # read evaluation file\n",
    "        evaluation_file = f\"recommendation/log/{today_format}_{model_name}_{dataset_name}/test_result.json\"\n",
    "\n",
    "    else:\n",
    "        evaluation_file = f\"recommendation/log/{model_name}/test_result.json\"\n",
    "\n",
    "    with open(evaluation_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        metrics = json.load(f)\n",
    "    # format metrics as table for visualisation\n",
    "    table = {}\n",
    "    for metric_key, value in metrics.items():\n",
    "        metric, k = metric_key.split(\"@\")\n",
    "        if metric not in table:\n",
    "            table[metric] = {}\n",
    "        table[metric][f\"@{k}\"] = value\n",
    "\n",
    "    df = pd.DataFrame(table).T\n",
    "    df = df[sorted(df.columns, key=lambda x: int(x[1:]))]\n",
    "\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0yFRo1djsdS"
   },
   "source": [
    "# üß∞ FairDiverse Tutorial\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "## **1. Add New Dataset üìÅ**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: ‚¨áÔ∏è Download the Dataset from LastFM\n",
    "Download link: [LastFM Dataset](https://files.grouplens.org/datasets/hetrec2011/hetrec2011-lastfm-2k.zip) OR execute cell below\n",
    "\n",
    "#### What if the Dataset is Not in RecBole Format?\n",
    "Follow the steps [here](https://recbole.io/docs/user_guide/usage/running_new_dataset.html) in order to convert your data files to RecBole format which uses atomic files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-16 14:12:47--  https://files.grouplens.org/datasets/hetrec2011/hetrec2011-lastfm-2k.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2589075 (2.5M) [application/zip]\n",
      "Saving to: ‚Äòrecommendation/dataset/LastFM.zip‚Äô\n",
      "\n",
      "recommendation/data 100%[===================>]   2.47M  2.59MB/s    in 1.0s    \n",
      "\n",
      "2025-06-16 14:12:48 (2.59 MB/s) - ‚Äòrecommendation/dataset/LastFM.zip‚Äô saved [2589075/2589075]\n",
      "\n",
      "Archive:  recommendation/dataset/LastFM.zip\n",
      "  inflating: recommendation/dataset/LastFM/user_taggedartists-timestamps.dat  \n",
      "  inflating: recommendation/dataset/LastFM/artists.dat  \n",
      "  inflating: recommendation/dataset/LastFM/user_artists.dat  \n"
     ]
    }
   ],
   "source": [
    "!wget https://files.grouplens.org/datasets/hetrec2011/hetrec2011-lastfm-2k.zip -O recommendation/dataset/LastFM.zip -nc\n",
    "!mkdir -p recommendation/dataset/LastFM\n",
    "!unzip -n recommendation/dataset/LastFM.zip -d recommendation/dataset/LastFM\n",
    "!rm recommendation/dataset/LastFM.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### üé¨ MovieLens Dataset\n",
    "In this notebook we will use the MovieLens Dataset as an example.\n",
    "\n",
    "GroupLens Research has collected and made available rating data sets from the MovieLens web site (https://movielens.org). This dataset describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service.\n",
    "\n",
    "**Download the MovieLens dataset from RecBole: [MovieLens Dataset (RecBole processed)](https://drive.google.com/file/d/1G7_XhdSi1BhIvRETg0nN0O5tuOvbEs65/view?usp=drive_link)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"LastFM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2:** Place the dataset files under `~/recommendation/dataset/LastFM`\n",
    "\n",
    "We rename the downloaded files to match the RecBole format. The directory structure should look like this:\n",
    "\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "        ‚îî‚îÄ‚îÄ dataset\n",
    "            ‚îî‚îÄ‚îÄ LastFM\n",
    "                ‚îú‚îÄ‚îÄ artists.dat -> LastFM.item\n",
    "                ‚îú‚îÄ‚îÄ user_artists.dat -> LastFM.inter\n",
    "                ‚îú‚îÄ‚îÄ user_taggedartists-timestamps.dat -> LastFM.user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv recommendation/dataset/LastFM/artists.dat recommendation/dataset/LastFM/LastFM.item\n",
    "!mv recommendation/dataset/LastFM/user_artists.dat recommendation/dataset/LastFM/LastFM.inter\n",
    "!mv recommendation/dataset/LastFM/user_taggedartists-timestamps.dat recommendation/dataset/LastFM/LastFM.user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"recommendation/dataset/{dataset_name}\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "# move the dataset files in the folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Content üìÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**User Data**\n",
    "\n",
    "The file user_taggedartists-timestamps.dat comprising the attributes of the user tagged artists.\n",
    "\n",
    "Each record/line in the file has the following fields: \n",
    " \n",
    "- `userID`: the id of the users.\n",
    "- `artistID`: the id of the artists.\n",
    "- `tagID`: the id of the tags.\n",
    "- `timestamp`: the timestamp of the user interaction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sample\n",
      "   userID  artistID  tagID      timestamp\n",
      "0       2        52     13  1238536800000\n",
      "1       2        52     15  1238536800000\n",
      "2       2        52     18  1238536800000\n",
      "3       2        52     21  1238536800000\n",
      "4       2        52     41  1238536800000\n"
     ]
    }
   ],
   "source": [
    "user_path = os.path.join(data_path, f\"{dataset_name}.user\")\n",
    "user_data = pd.read_csv(user_path,delimiter='\\t')\n",
    "\n",
    "print(f\"Data Sample\")\n",
    "print(user_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Users: 1892\n"
     ]
    }
   ],
   "source": [
    "num_users = user_data[\"userID\"].nunique()\n",
    "print(f\"Total Users: {num_users}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Item Data**\n",
    "\n",
    "The file artists.dat comprising the attributes of the artists.\n",
    "\n",
    "Each record/line in the file has the following fields: \n",
    " \n",
    "- `artistID`: the id of the artists.\n",
    "- `name`: the name of the artists.\n",
    "- `url`: the url of the artists.\n",
    "- `pictureURL`: the picture url of the artists.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   artistID          name                                    url  \\\n",
      "0         1  MALICE MIZER  http://www.last.fm/music/MALICE+MIZER   \n",
      "1         1  MALICE MIZER  http://www.last.fm/music/MALICE+MIZER   \n",
      "2         1  MALICE MIZER  http://www.last.fm/music/MALICE+MIZER   \n",
      "3         1  MALICE MIZER  http://www.last.fm/music/MALICE+MIZER   \n",
      "4         1  MALICE MIZER  http://www.last.fm/music/MALICE+MIZER   \n",
      "\n",
      "                                        pictureURL   tagID  \n",
      "0  http://userserve-ak.last.fm/serve/252/10808.jpg   552.0  \n",
      "1  http://userserve-ak.last.fm/serve/252/10808.jpg  1219.0  \n",
      "2  http://userserve-ak.last.fm/serve/252/10808.jpg   139.0  \n",
      "3  http://userserve-ak.last.fm/serve/252/10808.jpg   141.0  \n",
      "4  http://userserve-ak.last.fm/serve/252/10808.jpg  2850.0  \n"
     ]
    }
   ],
   "source": [
    "item_path = os.path.join(data_path, f\"{dataset_name}.item\")\n",
    "item_data = pd.read_csv(item_path, delimiter='\\t', encoding='latin-1')\n",
    "item_data.rename(columns={'id': 'artistID'}, inplace=True)\n",
    "\n",
    "# Add the first tagID from the user data at the end of the item data where the artistID matches\n",
    "item_data = item_data.merge(user_data[['artistID', 'tagID']], on='artistID', how='left')\n",
    "# Convert tagID to int\n",
    "item_data['tagID'] = item_data['tagID'].dropna().astype(int)\n",
    "\n",
    "print(item_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Items: 17632\n"
     ]
    }
   ],
   "source": [
    "num_items = item_data[\"artistID\"].nunique()\n",
    "print(f\"Total Items: {num_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save item_data\n",
    "item_data.to_csv(item_path,sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Interaction Data**\n",
    "\n",
    "The file user_artists.dat comprising the weight of the user interaction with the artists.\n",
    "\n",
    "Each record/line in the file has the following fields: \n",
    "\n",
    "userID\tartistID\tweight\n",
    "- `userID`: the id of the users.\n",
    "- `artistID`: the id of the artists.\n",
    "- `weight`: the weight of the user interaction with the artist.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userID  artistID  weight      timestamp\n",
      "1       2        52   11690  1238536800000\n",
      "2       2        52   11690  1238536800000\n",
      "3       2        52   11690  1238536800000\n",
      "4       2        52   11690  1238536800000\n",
      "5       2        52   11690  1238536800000\n"
     ]
    }
   ],
   "source": [
    "interaction_path = os.path.join(data_path, f\"{dataset_name}.inter\")\n",
    "interaction_data = pd.read_csv(interaction_path,delimiter='\\t')\n",
    "\n",
    "# remove from interaction data items which were dropped\n",
    "interaction_data = interaction_data[interaction_data['artistID'].isin(item_data['artistID'])]\n",
    "\n",
    "# Add the weight from the user data to the interaction data, at the end of a row if userID and artistID match\n",
    "interaction_data = interaction_data.merge(user_data, on=['userID', 'artistID'], how='left')\n",
    "# Drop tagID column\n",
    "interaction_data = interaction_data[['userID', 'artistID', 'weight', 'timestamp']]\n",
    "# Drop NaN values in the timestamp column\n",
    "interaction_data = interaction_data.dropna(subset=['timestamp'])\n",
    "# Convert timestamp to integer\n",
    "interaction_data['timestamp'] = interaction_data['timestamp'].astype(int)\n",
    "\n",
    "print(interaction_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items interacted with:  6854\n",
      "Number of users who performed an interaction:  1824\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of items interacted with: \", len(interaction_data[\"artistID\"].unique()))\n",
    "print(\"Number of users who performed an interaction: \", len(interaction_data[\"userID\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3899</th>\n",
       "      <td>203165</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3900</th>\n",
       "      <td>227829</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3901</th>\n",
       "      <td>320725</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3902</th>\n",
       "      <td>324663</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3903</th>\n",
       "      <td>352698</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3904 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      weight    0\n",
       "0          1  126\n",
       "1          2  142\n",
       "2          3   80\n",
       "3          4   58\n",
       "4          5   67\n",
       "...      ...  ...\n",
       "3899  203165    6\n",
       "3900  227829    4\n",
       "3901  320725    2\n",
       "3902  324663   29\n",
       "3903  352698    3\n",
       "\n",
       "[3904 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distirbution of ratings\n",
    "interaction_data.groupby(\"weight\").size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save interaction_data\n",
    "interaction_data.to_csv(interaction_path, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3:** Create a configuration file for the dataset under `~/recommendation/properties/dataset/ml-100k.yaml`\n",
    "\n",
    "```yaml\n",
    "{\n",
    "    user_id: user_id:token, # column name of the user ID\n",
    "    item_id: item_id:token, # column name of the item ID, in this case we recommend movies\n",
    "    group_id: first_class:token, # column name of the groups to be considered for fairness, in this case we consider the genres of the movie\n",
    "    label_id: rating:float, # column name for the label, indicating the interest of the user in the item\n",
    "    timestamp: timestamp:float, # column name for the timestamp of when the interaction happened\n",
    "    text_id: movie_title:token_seq, # column name for the text ID of the item (e.g. movie name, book title)\n",
    "    label_threshold: 3, # if label exceed the value will be regarded as 1, otherwise, it will be accounted into 0 --> we consider a positive recommendation if a user rated a movie with a value higher than 3\n",
    "    item_domain: movie, # description of the dataset domain (e.g. movie, music, jobs etc.)\n",
    "\n",
    "   item_val: 5, # keep items which have at least this number of interactions\n",
    "   user_val: 5, # keep users who have at least this number of interactions\n",
    "   group_val: 5, # keep groups which have at least this number of interactions\n",
    "   group_aggregation_threshold: 15, ##If the number of items owned by a group is less than this value, those groups will be merged into a single group called the 'infrequent group'. For example, Fantasy, War, Musician, ... will be merged into one group called 'infrequent group', as the number of items belonging to this group is under the threshold.\n",
    "   sample_size: 1.0, ###Sample ratio of the whole dataset to form a new subset dataset for training.\n",
    "   valid_ratio: 0.1, ### Samples to be used for validation\n",
    "   test_ratio: 0.1, ### Samples to be used for test\n",
    "   reprocess: True, ##do you need to re-process the dataset according to your personalized requirements\n",
    "   sample_num: 300, # needs to be higher than the max number of positive samples per user\n",
    "   history_length: 20, # length of historical interactions of a user - [item_1, item_2, item_3, ...] to be considered\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = {\n",
    "    \"user_id\": \"userID\",\n",
    "    \"item_id\": \"artistID\",\n",
    "    \"group_id\": \"tagID\",\n",
    "    \"label_id\": \"weight\",\n",
    "    \"timestamp\": \"timestamp\",\n",
    "    \"text_id\": \"name\",\n",
    "    \"label_threshold\": 3,\n",
    "    \"item_domain\": \"music\",\n",
    "\n",
    "\n",
    "    \"item_val\": 5,\n",
    "    \"user_val\": 5,\n",
    "    \"group_val\": 5,\n",
    "    \"group_aggregation_threshold\": 15,\n",
    "    \"sample_size\": 1.0,\n",
    "    \"valid_ratio\": 0.1,\n",
    "    \"test_ratio\": 0.1,\n",
    "    \"reprocess\": True,\n",
    "    \"sample_num\": 350,\n",
    "    \"history_length\": 20,\n",
    "}\n",
    "\n",
    "with open(f\"./recommendation/properties/dataset/{dataset_name}.yaml\", \"w+\") as file:\n",
    "    yaml.dump(config_data, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dataset as a choice in main.py\n",
    "with open(\"main.py\", \"r\") as f:\n",
    "    content = f.read()\n",
    "content = content.replace(\"choices=[\\\"steam\\\", \\\"clueweb09\\\", \\\"compas\\\"]\", f\"choices=[\\\"steam\\\", \\\"clueweb09\\\", \\\"compas\\\", \\\"{dataset_name}\\\"]\")\n",
    "with open(\"main.py\", \"w\") as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "## **2. Base Recommender System**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "To check that the set-up of the new dataset works well, let's train a base recommender system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 1:** Define your training configuration file: `~/recommendation/train-base-model.yaml`\n",
    "\n",
    "You can change parameters specific to each model in the following configuration file: `recommendation/properties/models/<model_name>.yaml`\n",
    "\n",
    "```yaml\n",
    "{\n",
    "   ############base model#########################\n",
    "   model: SASRec, # define the model to train\n",
    "   data_type: 'sequential', #[point, pair, sequential] # define the data_type needed by the model during training SASRec is a sequnetial recommender system, expecting the data_type to be 'sequential'\n",
    "   #############################################################\n",
    "\n",
    "   ##Should the preprocessing be redone based on the new parameters instead of using the cached files in ~/recommendation/process_dataset######\n",
    "   reprocess: True,\n",
    "   ###############################################\n",
    "\n",
    "  ####fair-rank model settings --> set all to False as we want to only train the base model without any fairness/diversity intervention\n",
    "   fair-rank: False, ##if you want to run a fair-rank module on the base models, you should set the value as True\n",
    "\n",
    "  # LLM recommendation setting\n",
    "   use_llm: False,\n",
    "\n",
    "  #############log name, it will store the evaluation result in ~log/your_log_name/\n",
    "   log_name: \"SASRec_ml-100k\",\n",
    "  #################################################\n",
    "\n",
    "   ###########################training parameters################################\n",
    "   device: cpu,\n",
    "   epoch: 20,\n",
    "   batch_size: 64,\n",
    "   learning_rate: 0.001,\n",
    "   ###########################################################################\n",
    "\n",
    "\n",
    "   ###################################evaluation parameters: overwrite from ~/properties/evaluation.yaml######################################\n",
    "   mmf_eval_ratio: 0.5,\n",
    "   decimals: 4,\n",
    "   eval_step: 5,\n",
    "   eval_type: 'ranking',\n",
    "   watch_metric: 'mmf@20',\n",
    "   topk: [ 5,10,20 ], # if you choose the ranking settings, you can choose your top-k list\n",
    "   store_scores: True, #If set true, the all relevance scores will be stored in the ~/log/your_name/ for post-processing\n",
    "   fairness_metrics: ['MinMaxRatio', \"MMF\", \"GINI\", \"Entropy\"],\n",
    "   fairness_type: \"Exposure\", # [\"Exposure\", \"Utility\"], where Exposure only computes the exposure of item group while utility computes the ranking score of item groups\n",
    "   ###########################################################################\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with the baselines models provided by FairDiverse\n",
    "base_model_name = \"SASRec\"\n",
    "config_base = {\n",
    "    # ############ base model #########################\n",
    "    \"model\": f\"{base_model_name}\",\n",
    "    \"data_type\": \"sequential\",\n",
    "\n",
    "    # Should preprocessing be redone (ignore cache)?\n",
    "    \"reprocess\": True,\n",
    "\n",
    "    # Fair-rank settings !!! Don't change - needs to be set to False for running the base model !!!\n",
    "    \"fair-rank\": False,  # run fair-rank module or not\n",
    "\n",
    "    # LLM recommendation setting !!! Don't change - needs to be set to False for running the base model !!!\n",
    "    \"use_llm\": False,\n",
    "\n",
    "    # Log name (results will be stored in ~/log/{log_name}/)\n",
    "    \"log_name\": f\"{base_model_name}_{dataset_name}\",\n",
    "\n",
    "    # ############# training parameters #################\n",
    "    \"device\": \"cpu\",\n",
    "    \"epoch\": 20,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "\n",
    "    # ############# evaluation parameters #################\n",
    "    \"mmf_eval_ratio\": 0.5,\n",
    "    \"decimals\": 4,\n",
    "    \"eval_step\": 5,\n",
    "    \"eval_type\": \"ranking\",\n",
    "    \"watch_metric\": \"mmf@20\",\n",
    "    \"topk\": [5, 10, 20],\n",
    "    \"store_scores\": True,\n",
    "    \"fairness_metrics\": [\"MinMaxRatio\", \"MMF\", \"GINI\", \"Entropy\"],\n",
    "    \"fairness_type\": \"Exposure\"  # [\"Exposure\", \"Utility\"]\n",
    "}\n",
    "\n",
    "with open(f\"./recommendation/train-base-model.yaml\", \"w\") as file:\n",
    "    yaml.dump(config_base, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 2: Run the Base Recommender System**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your training config...\n",
      "{'model': 'SASRec', 'data_type': 'sequential', 'reprocess': True, 'fair-rank': False, 'use_llm': False, 'log_name': 'SASRec_LastFM', 'device': 'cpu', 'epoch': 20, 'batch_size': 64, 'learning_rate': 0.001, 'mmf_eval_ratio': 0.5, 'decimals': 4, 'eval_step': 5, 'eval_type': 'ranking', 'watch_metric': 'mmf@20', 'topk': [5, 10, 20], 'store_scores': True, 'fairness_metrics': ['MinMaxRatio', 'MMF', 'GINI', 'Entropy'], 'fairness_type': 'Exposure', 'dataset': 'LastFM', 'stage': 'in-processing', 'task': 'recommendation'}\n",
      "your args: Namespace(task='recommendation', stage='in-processing', dataset='LastFM', train_config_file='train-base-model.yaml')\n",
      "process config:\n",
      "{'item_val': 5, 'user_val': 5, 'group_val': 5, 'group_aggregation_threshold': 15, 'sample_size': 1.0, 'valid_ratio': 0.1, 'test_ratio': 0.1, 'reprocess': True, 'sample_num': 350, 'history_length': 20, 'user_id': 'userID', 'item_id': 'artistID', 'group_id': 'tagID', 'label_id': 'weight', 'timestamp': 'timestamp', 'text_id': 'name', 'label_threshold': 3, 'item_domain': 'music', 'model': 'SASRec', 'data_type': 'sequential', 'fair-rank': False, 'use_llm': False, 'log_name': 'SASRec_LastFM', 'device': 'cpu', 'epoch': 20, 'batch_size': 64, 'learning_rate': 0.001, 'mmf_eval_ratio': 0.5, 'decimals': 4, 'eval_step': 5, 'eval_type': 'ranking', 'watch_metric': 'mmf@20', 'topk': [5, 10, 20], 'store_scores': True, 'fairness_metrics': ['MinMaxRatio', 'MMF', 'GINI', 'Entropy'], 'fairness_type': 'Exposure', 'dataset': 'LastFM', 'stage': 'in-processing', 'task': 'recommendation'}\n",
      "start to process data...\n",
      "184941\n",
      "origin:----------item number: 6854 user number:1824 group_num:9718 total interactions:20665\n",
      "start to merge data.....\n",
      "processing item val\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/github/school/master1/artifical-intelligence/recommender-systems/BSARec/FairDiverse/fairdiverse/main.py\", line 39, in <module>\n",
      "    trainer.train()\n",
      "  File \"/media/github/school/master1/artifical-intelligence/recommender-systems/BSARec/FairDiverse/fairdiverse/recommendation/trainer.py\", line 143, in train\n",
      "    state = Process(self.dataset, self.train_config)\n",
      "  File \"/media/github/school/master1/artifical-intelligence/recommender-systems/BSARec/FairDiverse/fairdiverse/recommendation/process_dataset.py\", line 165, in Process\n",
      "    frames = frames.groupby(uid_field, group_keys=False).apply(get_previous_items)\n",
      "  File \"/home/dev/miniconda3/envs/bsarec/lib/python3.9/site-packages/pandas/core/groupby/groupby.py\", line 1824, in apply\n",
      "    result = self._python_apply_general(f, self._selected_obj)\n",
      "  File \"/home/dev/miniconda3/envs/bsarec/lib/python3.9/site-packages/pandas/core/groupby/groupby.py\", line 1885, in _python_apply_general\n",
      "    values, mutated = self._grouper.apply_groupwise(f, data, self.axis)\n",
      "  File \"/home/dev/miniconda3/envs/bsarec/lib/python3.9/site-packages/pandas/core/groupby/ops.py\", line 919, in apply_groupwise\n",
      "    res = f(group)\n",
      "  File \"/media/github/school/master1/artifical-intelligence/recommender-systems/BSARec/FairDiverse/fairdiverse/recommendation/process_dataset.py\", line 154, in get_previous_items\n",
      "    for _, row in sub_df.iterrows():\n",
      "  File \"/home/dev/miniconda3/envs/bsarec/lib/python3.9/site-packages/pandas/core/frame.py\", line 1559, in iterrows\n",
      "    s = klass(v, index=columns, name=k).__finalize__(self)\n",
      "  File \"/home/dev/miniconda3/envs/bsarec/lib/python3.9/site-packages/pandas/core/series.py\", line 584, in __init__\n",
      "    data = sanitize_array(data, index, dtype, copy)\n",
      "  File \"/home/dev/miniconda3/envs/bsarec/lib/python3.9/site-packages/pandas/core/construction.py\", line 604, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/home/dev/miniconda3/envs/bsarec/lib/python3.9/site-packages/pandas/core/dtypes/cast.py\", line 1198, in maybe_infer_to_datetimelike\n",
      "    return lib.maybe_convert_objects(  # type: ignore[return-value]\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! python \"main.py\" --task recommendation --stage \"in-processing\" --dataset \"{dataset_name}\" --train_config_file \"train-base-model.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Output files**\n",
    "---\n",
    "\n",
    "**Processed Dataset Structure**\n",
    "\n",
    "The following files are generated during preprocessing and saved under `processed_dataset/ml-100k/`:\n",
    "\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "    ‚îî‚îÄ‚îÄprocessed_dataset/\n",
    "        ‚îî‚îÄ‚îÄ ml-100k/\n",
    "            ‚îú‚îÄ‚îÄ iid2pid.json              # Mapping from item ID to provider/group ID\n",
    "            ‚îú‚îÄ‚îÄ iid2text.json             # Mapping from item ID to textual representation (e.g., title)\n",
    "            ‚îú‚îÄ‚îÄ movie_lens.test.CTR       # Test set for click-through rate (CTR) evaluation\n",
    "            ‚îú‚îÄ‚îÄ movie_lens.test.ranking   # Test set for ranking evaluation\n",
    "            ‚îú‚îÄ‚îÄ movie_lens.train          # Training set\n",
    "            ‚îú‚îÄ‚îÄ movie_lens.valid.CTR      # Validation set for CTR evaluation\n",
    "            ‚îú‚îÄ‚îÄ movie_lens.valid.ranking  # Validation set for ranking evaluation\n",
    "            ‚îî‚îÄ‚îÄ process_config.yaml       # Configuration used during preprocessing\n",
    "```\n",
    "**Log Output Directory Structure**\n",
    "\n",
    "After training, the following files are saved under the `log/` directory:\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "    ‚îî‚îÄ‚îÄlog/\n",
    "        ‚îî‚îÄ‚îÄ 2025-5-20_SASRec_ml-100k/\n",
    "            ‚îú‚îÄ‚îÄ best_model.pth         # Saved PyTorch model weights\n",
    "            ‚îú‚îÄ‚îÄ config.yaml            # Configuration used for training\n",
    "            ‚îú‚îÄ‚îÄ ranking_scores.npz     # Numpy array of ranking scores\n",
    "            ‚îî‚îÄ‚îÄ test_result.json       # Evaluation metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "**Evaluation Results üìà**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation_results(base_model_name, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "## **3. Run Post-processing Model**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **3.1 With Input from FairDiverse**\n",
    "\n",
    "---\n",
    "\n",
    "Run the post-processing model on-top of the base recommender system that we have trained in Section 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 1:** Create a configuration file for running a post-processing intervention under \n",
    "You can change parameters specific to each model in the following configuration file: `recommendation/properties/models/<model_name>.yaml` \n",
    "```yaml\n",
    "{\n",
    "   ###############the ranking score stored path for the post-processing##################\n",
    "   ranking_store_path: \"ml-100k\", \n",
    "   #######################################################################################\n",
    "\n",
    "   ### !!! Don't change - needs to be set to False as we don't run a post-processing intervention !!!\n",
    "   model: \"CPFair\",\n",
    "   log_name: \"CPFair_ml-100k\",\n",
    "\n",
    "   #########################Evaluation parameters#########################################\n",
    "   topk: [5, 10, 20],\n",
    "   fairness_metrics: ['MinMaxRatio', \"MMF\", \"GINI\", \"Entropy\"],\n",
    "   fairness_type: \"Exposure\", # [\"Exposure\", \"Utility\"], where Exposure only computes the exposure of item group while utility computes the ranking score of item groups\n",
    "   #####################################################################################\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessing_model_name = \"CPFair\"\n",
    "today = date.today()\n",
    "today_format = f\"{today.year}-{today.month}-{today.day}\"\n",
    "\n",
    "config_model = {\n",
    "    \"ranking_store_path\": f\"{today_format}_{base_model_name}_{dataset_name}\",  # Path to the ranking score file (required for post-processing)\n",
    "\n",
    "    # Change to any of the supported post-processing methods in Fairdiverse\n",
    "    \"model\": f\"{postprocessing_model_name}\",\n",
    "     \"fair-rank\": True,\n",
    "\n",
    "    \"log_name\": f\"{postprocessing_model_name}_{dataset_name}\", # path to save the evaluation and the output\n",
    "\n",
    "    # Evaluation parameters\n",
    "    \"topk\": [5, 10, 20],\n",
    "    \"fairness_metrics\": [\"MinMaxRatio\", \"MMF\", \"GINI\", \"Entropy\"],\n",
    "    \"fairness_type\": \"Exposure\"  # \"Exposure\" computes exposure of item group; \"Utility\" computes score differences\n",
    "}\n",
    "\n",
    "with open(f\"./recommendation/postprocessing_with_fairdiverse.yaml\", \"w\") as file:\n",
    "    yaml.dump(config_model, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "**Step 2: Run the post-processing model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python \"main.py\" --task recommendation --stage \"post-processing\" --dataset \"{dataset_name}\" --train_config_file \"postprocessing_with_fairdiverse.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "**Evaluation Resultsüìà**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "### NDCG as a Measure of Utility Loss\n",
    "\n",
    "Here, **Normalized Discounted Cumulative Gain (NDCG)** is used to quantify the **loss in utility** resulting from the post-processing intervention.\n",
    "\n",
    "Specifically, it compares the ranking produced by **CP-Fair** with the original ranking of the **base model** (e.g., *SASRec*).\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\n",
    "\\text{Mean\\_NDCG@k} = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{DCG_u}{IDCG_u}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "-  *U* is the set of users,\n",
    "- **DCG** is computed based on the ranking produced by the post-processing intervention (e.g. CP-Fair),\n",
    "- **Ideal DCG** is computed based on the original ranking produced by the base model (e.g. SASRec).\n",
    "\n",
    "An NDCG closer to 1 indicates minimal loss in utility due to the intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "### Mean Utility Loss\n",
    "\n",
    "The **mean utility loss at rank k** across all users is defined as:\n",
    "\n",
    "$$\n",
    "U_{loss@k} = \\frac{1}{|U|} \\sum_{u \\in U} \\left[ \\frac{1}{k} \\left( \\sum_{i=1}^{k} \\text{score}_{base} {(u,i)} - \\sum_{i=1}^{k} \\text{score}_{post} {(u,i)} \\right) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- *U* is the set of users,\n",
    "- $ \\text{score}_{base} {(u,i)} $  is the score assigned to the *i-th* item in the **base model's** top-*k* ranking for user *u*,\n",
    "- $ \\text{score}_{post} {(u,i)} $ is the score of the *i-th* item in the **post-processing model's** top-*k* ranking for user *u*.\n",
    "\n",
    "This metric captures the **average per-item utility loss over all users**, reflecting how much the re-ranking procedure deviates from the base model in terms of utility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation results of post-processing model\n",
    "print_evaluation_results(postprocessing_model_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation results of the base model\n",
    "print_evaluation_results(base_model_name, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úÖ CP-Fair improves fairness and diversity metrics over the base model SASRec, with only a small drop in NDCG and utility loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **3.2 Withoout Input from FairDiverse**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "To simulate a scenario where you did not use FairDiverse to generate the required files let's rename the already generated folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename(\"recommendation/processed_dataset/ml-100k\", \"recommendation/processed_dataset/ml-100k_fairdiverse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "**Expected Data Format**\n",
    "\n",
    "If you want to use a model not supported by FairDiverse and run the evaluation metrics you need to have the following files:\n",
    "\n",
    "(1) `iid2pid.json` - Mapping from item ID to provider/group ID \n",
    "\n",
    "(2) `ranking_scores.npz` - Numpy array of ranking scores\n",
    "\n",
    "Otherwise use one of the Base Models or In-processing models supported by FairDiverse to generate those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Example of expected format for iid2pid.json file -- item_id:group_id\n",
    "iid2pid = {\"1488\": \"0\", \"42\": \"1\", \"2508\": \"2\", \"1084\": \"3\", \"1182\": \"0\", \"1468\": \"4\", \"2087\": \"3\", \"153\": \"0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Example of expected format for ranking_scores.npz file -- sparse matrix of users x items and the corresponding score\n",
    "\n",
    "n_users = 50\n",
    "n_items = 100\n",
    "user_item_matrix = np.random.rand(n_users, n_items)\n",
    "print(user_item_matrix)\n",
    "print(\"Shape:\", user_item_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 1:** Create the files needed for running the post-processing method.\n",
    "\n",
    "To simulate a scenarion where you did not use FairDiverse to create the files, take the files generated by the previous base model (e.g. SASRec) and follow the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 2:** Place `ranking_scores.npz` under `~/recommendation/log/ml-100k`\n",
    "\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "    ‚îî‚îÄ‚îÄlog/\n",
    "        ‚îî‚îÄ‚îÄ ml-100k/\n",
    "            ‚îú‚îÄ‚îÄ ranking_scores.npz     # Numpy array of ranking scores\n",
    "\n",
    "```\n",
    "#### **Step 3:** Place `iid2pid.json` under `~/recommendation/processed_dataset/ml-100k`\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "    ‚îî‚îÄ‚îÄprocessed_dataset/\n",
    "        ‚îî‚îÄ‚îÄ ml-100k/\n",
    "            ‚îú‚îÄ‚îÄ iid2pid.json    # Mapping from item ID to provider/group ID\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 4:** Save data configuration file `process_config.yaml` under `~/recommendation/processed_dataset/ml-100k`\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "    ‚îî‚îÄ‚îÄprocessed_dataset/\n",
    "        ‚îî‚îÄ‚îÄ ml-100k/\n",
    "            ‚îú‚îÄ‚îÄ process_config.yaml   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"recommendation/log/{dataset_name}\", exist_ok=True)\n",
    "os.makedirs(f\"recommendation/processed_dataset/{dataset_name}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this if you did not use FairDiverse to create the score file\n",
    "num_users = 822 # n rows of the matrix\n",
    "num_items = 1345 # n columns of the matrix\n",
    "num_groups = 14 # this should correspond to the unique values from iid2pid.json\n",
    "config_data[\"item_num\"] = num_items\n",
    "config_data[\"user_num\"] = num_users\n",
    "config_data[\"group_num\"] = num_groups\n",
    "\n",
    "print(config_data)\n",
    "with open(f\"recommendation/processed_dataset/{dataset_name}/process_config.yaml\", \"w\") as file:\n",
    "    yaml.dump(config_data, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 5:** Create a configuration file for running a post-processing intervention under \n",
    "You can change parameters specific to each model in the following configuration file: `recommendation/properties/models/<model_name>.yaml` \n",
    "```yaml\n",
    "{\n",
    "   ###############the ranking score stored path for the post-processing##################\n",
    "   ranking_store_path: \"ml-100k\", \n",
    "   #######################################################################################\n",
    "\n",
    "   ### !!! Don't change - needs to be set to False as we don't run a post-processing intervention !!!\n",
    "   model: \"CPFair\",\n",
    "   log_name: \"CPFair_ml-100k\",\n",
    "\n",
    "   #########################Evaluation parameters#########################################\n",
    "   topk: [5, 10, 20],\n",
    "   fairness_metrics: ['MinMaxRatio', \"MMF\", \"GINI\", \"Entropy\"],\n",
    "   fairness_type: \"Exposure\", # [\"Exposure\", \"Utility\"], where Exposure only computes the exposure of item group while utility computes the ranking score of item groups\n",
    "   #####################################################################################\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessing_model_name = \"CPFair\"\n",
    "\n",
    "config_model = {\n",
    "    \"ranking_store_path\": f\"{dataset_name}\",  # Path to the ranking score file (required for post-processing)\n",
    "\n",
    "    # Change to any of the supported post-processing methods in Fairdiverse\n",
    "    \"model\": f\"{postprocessing_model_name}\",\n",
    "    \"fair-rank\": True,\n",
    "\n",
    "    \"log_name\": f\"{postprocessing_model_name}_without_fairdiverse_{dataset_name}\", # path to save the evaluation and the output\n",
    "\n",
    "    # Evaluation parameters\n",
    "    \"topk\": [5, 10, 20],\n",
    "    \"fairness_metrics\": [\"MinMaxRatio\", \"MMF\", \"GINI\", \"Entropy\"],\n",
    "    \"fairness_type\": \"Exposure\"  # \"Exposure\" computes exposure of item group; \"Utility\" computes score differences\n",
    "}\n",
    "\n",
    "with open(f\"./recommendation/postprocessing_without_fairdiverse.yaml\", \"w\") as file:\n",
    "    yaml.dump(config_model, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 6:** Run the post-processing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python \"main.py\" --task recommendation --stage \"post-processing\" --dataset \"{dataset_name}\" --train_config_file \"postprocessing_without_fairdiverse.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "##### **Evaluation Results üìà** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDCG as a Measure of Utility Loss\n",
    "\n",
    "Here, **Normalized Discounted Cumulative Gain (NDCG)** is used to quantify the **loss in utility** resulting from the post-processing intervention.\n",
    "\n",
    "Specifically, it compares the ranking produced by **CP-Fair** with the original ranking of the **base model** (e.g., *SASRec*).\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\n",
    "\\text{Mean\\_NDCG@k} = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{DCG_u}{IDCG_u}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "-  *U* is the set of users,\n",
    "- **DCG** is computed based on the ranking produced by the post-processing intervention (e.g. CP-Fair),\n",
    "- **Ideal DCG** is computed based on the original ranking produced by the base model (e.g. SASRec).\n",
    "\n",
    "An NDCG closer to 1 indicates minimal loss in utility due to the intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Utility Loss\n",
    "\n",
    "The **mean utility loss at rank k** across all users is defined as:\n",
    "\n",
    "$$\n",
    "U_{loss@k} = \\frac{1}{|U|} \\sum_{u \\in U} \\left[ \\frac{1}{k} \\left( \\sum_{i=1}^{k} \\text{score}_{base} {(u,i)} - \\sum_{i=1}^{k} \\text{score}_{post} {(u,i)} \\right) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- *U* is the set of users,\n",
    "- $ \\text{score}_{base} {(u,i)} $  is the score assigned to the *i-th* item in the **base model's** top-*k* ranking for user *u*,\n",
    "- $ \\text{score}_{post} {(u,i)} $ is the score of the *i-th* item in the **post-processing model's** top-*k* ranking for user *u*.\n",
    "\n",
    "This metric captures the **average per-item utility loss over all users**, reflecting how much the re-ranking procedure deviates from the base model in terms of utility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation_results(postprocessing_model_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation_results(base_model_name, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "## **4. Run Evaluation üìà**\n",
    "\n",
    "---\n",
    "\n",
    "If you want to use a model not supported by FairDiverse and run the evaluation metrics you need to have the following files:\n",
    "\n",
    "(1) `iid2pid.json` - Mapping from item ID to provider/group ID \n",
    "\n",
    "(2) `ranking_scores.npz` - Numpy array of ranking scores\n",
    "\n",
    "To simulate a scenario where you did not use FairDiverse to generate the required files let's rename the already generated folder for the base model, and run again only the evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to run if you did this in Section 3.1\n",
    "# os.rename(\"recommendation/processed_dataset/ml-100k\", \"recommendation/processed_dataset/ml-100k_fairdiverse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Example of expected format for iid2pid.json file -- item_id:group_id\n",
    "iid2pid = {\"1488\": \"0\", \"42\": \"1\", \"2508\": \"2\", \"1084\": \"3\", \"1182\": \"0\", \"1468\": \"4\", \"2087\": \"3\", \"153\": \"0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Example of expected format for ranking_scores.npz file -- sparse matrix of users x items and the corresponding score\n",
    "\n",
    "users = 50\n",
    "items = 100\n",
    "user_item_matrix = np.random.rand(users, items)\n",
    "print(user_item_matrix)\n",
    "print(\"Shape:\", user_item_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 1:** Create the files needed for running the evaluation\n",
    "\n",
    "To simulate a scenarion where you did not use FairDiverse to create the files, take the files generated by the previous base model (e.g. SASRec) and follow the steps below.\n",
    "#### **Step 2:** Place `ranking_scores.npz` under `~/recommendation/log/SASRec_ml-100k`\n",
    "\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "    ‚îî‚îÄ‚îÄlog/\n",
    "        ‚îî‚îÄ‚îÄ ml-100k/\n",
    "            ‚îú‚îÄ‚îÄ ranking_scores.npz     # Numpy array of ranking scores\n",
    "\n",
    "```\n",
    "#### **Step 3:** Place `iid2pid.json` under `~/recommendation/processed_dataset/ml-100k`\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "    ‚îî‚îÄ‚îÄprocessed_dataset/\n",
    "        ‚îî‚îÄ‚îÄ ml-100k/\n",
    "            ‚îú‚îÄ‚îÄ iid2pid.json    # Mapping from item ID to provider/group ID\n",
    "```\n",
    "\n",
    "\n",
    "#### **Step 4:** Save data configuration file `process_config.yaml` under `~/recommendation/processed_dataset/ml-100k`\n",
    "```text\n",
    "fairdiverse\n",
    "‚îî‚îÄ‚îÄ recommendation\n",
    "    ‚îî‚îÄ‚îÄprocessed_dataset/\n",
    "        ‚îî‚îÄ‚îÄ ml-100k/\n",
    "            ‚îú‚îÄ‚îÄ process_config.yaml   \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"recommendation/log/{base_model_name}_{dataset_name}\", exist_ok=True)\n",
    "os.makedirs(f\"recommendation/processed_dataset/{dataset_name}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this if you did not use FairDiverse to create the score file\n",
    "num_users = 822 # n rows of the matrix\n",
    "num_items = 1345 # n columns of the matrix\n",
    "num_groups = 14 # this should correspond to the unique values from iid2pid.json\n",
    "config_data[\"item_num\"] = num_items\n",
    "config_data[\"user_num\"] = num_users\n",
    "config_data[\"group_num\"] = num_groups\n",
    "\n",
    "print(config_data)\n",
    "with open(f\"recommendation/processed_dataset/{dataset_name}/process_config.yaml\", \"w\") as file:\n",
    "    yaml.dump(config_data, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 5:** Create a configuration file for the evaluation\n",
    "```yaml\n",
    "{\n",
    "   ###############the ranking score stored path for the post-processing##################\n",
    "   ranking_store_path: \"SASRec_ml-100k\", \n",
    "   #######################################################################################\n",
    "\n",
    "   ### !!! Don't change - needs to be set to False as we don't run a post-processing intervention !!!\n",
    "   model: False,\n",
    "   use_llm: False,\n",
    "   ###############eval output path##################\n",
    "   log_name: \"eval_ml-100k\",\n",
    "\n",
    "   #########################Evaluation parameters#########################################\n",
    "   topk: [5, 10, 20],\n",
    "   fairness_metrics: ['MinMaxRatio', \"MMF\", \"GINI\", \"Entropy\"],\n",
    "   fairness_type: \"Exposure\", # [\"Exposure\", \"Utility\"], where Exposure only computes the exposure of item group while utility computes the ranking score of item groups\n",
    "   #####################################################################################\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_eval = {\n",
    "    \"ranking_store_path\": f\"{base_model_name}_{dataset_name}\",  # Path to the ranking score file (required for post-processing)\n",
    "\n",
    "    # Do not change ‚Äî no post-processing model used, and no base model used as we want to just perform evaluation\n",
    "    \"fair-rank\": False,\n",
    "    # output file for evaluation results\n",
    "    \"log_name\": f\"eval_{base_model_name}_without_fairdiverse_{dataset_name}\", # path to save the evaluation\n",
    "\n",
    "    # Evaluation parameters\n",
    "    \"topk\": [5, 10, 20],\n",
    "    \"fairness_metrics\": [\"MinMaxRatio\", \"MMF\", \"GINI\", \"Entropy\"],\n",
    "    \"fairness_type\": \"Exposure\"  # \"Exposure\" computes exposure of item group; \"Utility\" computes score differences\n",
    "}\n",
    "\n",
    "\n",
    "with open(f\"./recommendation/evaluation.yaml\", \"w\") as file:\n",
    "    yaml.dump(config_eval, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "#### **Step 6:** Run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python \"main.py\" --task recommendation --stage \"post-processing\" --dataset \"{dataset_name}\" --train_config_file \"evaluation.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "##### **Evaluation Resultsüìà**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation_results(f\"eval_{base_model_name}_without_fairdiverse\", dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "## **5. Add Post-processing Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "RAIF is a reranking approach based on MILP that selects N items from K candidates for each user. Its fairness objective aims to balance the overall exposure among different item groups. Now we introduce a new optimization objective‚Äîminimizing the disparity in average exposure across groups‚Äîand refer to this enhanced method as RAIFPro. These are the steps for adding RAIFPro to FairDiverse. You could also add your own post-processing model in the similar way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Create a python file inside: `fairdiverse/recommendation/rerank_model` which should have the name of the model (e.g. RAIFPro.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(os.path.join(\"recommendation/rerank_model\", \"RAIFPro.py\"), 'a').close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "**Step 2:** Implement a class which inherits `Abstract_Reranker` with the name of the model (e.g. RAIFPro). You can use the common parameters within the `Abstract_Reranker` class.\n",
    "\n",
    "Input:\n",
    "\n",
    "    relevance: numpy.ndarray, shape (num_users, num_items)\n",
    "        A 2D array where each row corresponds to a user and contains item relevance scores.\n",
    "    topk: int\n",
    "        The number of top-ranked items to select per user.\n",
    "\n",
    "\n",
    "Output: \n",
    "\n",
    "    rerank_list: list of list of int, shape (num_users, size)\n",
    "        A list where each entry contains exactly `size` selected items for a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile recommendation/rerank_model/RAIFPro.py\n",
    "\n",
    "import numpy as np\n",
    "from .Abstract_Reranker import Abstract_Reranker\n",
    "from gurobipy import Model, GRB, quicksum\n",
    "\n",
    "r\"\"\"\n",
    "RAIFPro changes the RAIF's optimization objective to minimizing the disparity in average exposure across groups.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_results(num_users, size, topk, solution, topk_items):\n",
    "    \"\"\"\n",
    "    Converts the solution matrix into selected item lists for multiple users.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    num_users: int\n",
    "        The number of users.\n",
    "    size: int\n",
    "        The expected number of items per user in the final rerank list.\n",
    "    topk: int\n",
    "        The number of candidate items per user.\n",
    "    solution: numpy.ndarray, shape (num_users, topk)\n",
    "        A matrix indicating the final selected items.\n",
    "    topk_items: list of list of int, shape (num_users, topk)\n",
    "        A list where each entry contains candidate item IDs corresponding to a user.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    rerank: list of list of int, shape (num_users, size)\n",
    "        A list where each entry contains exactly `size` selected items for a user.\n",
    "    \"\"\"\n",
    "\n",
    "    rerank = []\n",
    "    for i in range(num_users):\n",
    "\n",
    "        rerank_user = []\n",
    "        for j in range(topk):\n",
    "            if solution[i, j] > 0.5:\n",
    "                rerank_user.append(topk_items[i][j])\n",
    "\n",
    "        assert len(rerank_user) == size\n",
    "        rerank.append([int(x) for x in rerank_user])\n",
    "\n",
    "    return rerank\n",
    "\n",
    "def load_ranking_matrices(relevance, topk):\n",
    "    \"\"\"\n",
    "    Generates ranking matrices by selecting the top-k relevant items for each user.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    relevance: numpy.ndarray, shape (num_users, num_items)\n",
    "        A 2D array where each row corresponds to a user and contains item relevance scores.\n",
    "    topk: int\n",
    "        The number of top-ranked items to select per user.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    topk_items: numpy.ndarray, shape (num_users, topk)\n",
    "        A 2D array where each row contains the indices of the top-k items for the corresponding user.\n",
    "    topk_scores: numpy.ndarray, shape (num_users, topk)\n",
    "        A 2D array where each row contains the relevance scores of the selected top-k items.\n",
    "    num_users: int\n",
    "        The total number of users.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    num_users, num_items = relevance.shape\n",
    "\n",
    "    topk_items = np.zeros((num_users, topk), dtype=int)\n",
    "    topk_scores = np.zeros((num_users, topk))\n",
    "\n",
    "    for user_idx in range(num_users):\n",
    "        # Get the indices of the items sorted by their relevance score in descending order\n",
    "        sorted_indices = np.argsort(relevance[user_idx])[::-1]\n",
    "\n",
    "        # Select the top k indices and corresponding scores\n",
    "        topk_items[user_idx] = sorted_indices[:topk]\n",
    "        topk_scores[user_idx] = relevance[user_idx, sorted_indices[:topk]]\n",
    "\n",
    "    return topk_items, topk_scores, num_users\n",
    "\n",
    "def read_item_index(total_users, topk, no_item_groups, item_group_map, topk_items):\n",
    "    \"\"\"\n",
    "    Creates a binary indicator matrix that maps items to their respective item groups.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    total_users: int\n",
    "        The total number of users.\n",
    "    topk: int\n",
    "        The number of candidate items per user.\n",
    "    no_item_groups: int\n",
    "        The total number of item groups.\n",
    "    item_group_map: dict\n",
    "        A dictionary mapping item indices to their corresponding group IDs.\n",
    "    topk_items: list of list of int, shape (total_users, topk)\n",
    "        A list where each entry contains candidate item IDs corresponding to a user.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    Ihelp: numpy.ndarray, shape (total_users, topk, no_item_groups)\n",
    "        A binary 3D array where `Ihelp[uid][lid][k] = 1` if the `lid`-th item for user `uid`\n",
    "        belongs to item group `k`, otherwise `0`.\n",
    "    \"\"\"\n",
    "\n",
    "    Ihelp = np.zeros((total_users, topk, no_item_groups))\n",
    "    for uid in range(total_users):\n",
    "        for lid in range(topk):\n",
    "            for k in range(no_item_groups):\n",
    "                top_ = topk_items[uid][lid]\n",
    "                if top_ in item_group_map.keys():\n",
    "                    if item_group_map[topk_items[uid][lid]] == k:\n",
    "                        Ihelp[uid][lid][k] = 1\n",
    "\n",
    "    return Ihelp\n",
    "\n",
    "def fairness_optimisation(total_users, alpha, size, topk, group_num, Ihelp, topk_scores, mean):\n",
    "    \"\"\"\n",
    "    Solves a fairness-aware ranking optimization problem using Gurobi.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    total_users: int\n",
    "        The total number of users.\n",
    "    alpha: float\n",
    "        The fairness regularization parameter. A higher alpha increases fairness consideration.\n",
    "    size: int\n",
    "        The number of items to be selected per user.\n",
    "    topk: int\n",
    "        The number of candidate items per user.\n",
    "    group_num: int\n",
    "        The number of item groups.\n",
    "    Ihelp: numpy.ndarray, shape (total_users, topk, group_num)\n",
    "        A binary indicator matrix.\n",
    "    topk_scores: numpy.ndarray, shape (total_users, topk)\n",
    "        A 2D relevance score matrix.\n",
    "    mean: list\n",
    "        The ideal exposure across item groups.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    solution: numpy.ndarray, shape (num_users, topk)\n",
    "        A matrix indicating the final selected items.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Running RAIFPro, {format(alpha, 'f')}\")\n",
    "    # V1: No. of users\n",
    "    # V2: No. of top items (topk)\n",
    "    # V4: no. of item groups\n",
    "    V1, V2, V4 = range(total_users), range(topk), range(group_num)\n",
    "\n",
    "    # initiate model\n",
    "    model = Model()\n",
    "\n",
    "    W = model.addVars(V1, V2, vtype=GRB.BINARY)\n",
    "    item_group = model.addVars(V4, vtype=GRB.CONTINUOUS)\n",
    "    item_fair = model.addVar(vtype=GRB.CONTINUOUS)\n",
    "    abs_diff = model.addVars(V4, lb=0, name=\"abs_diff\")\n",
    "\n",
    "    model.setObjective(quicksum(topk_scores[i][j] * W[i, j] for i in V1 for j in V2) - alpha * item_fair, GRB.MAXIMIZE)\n",
    "\n",
    "    for i in V1:\n",
    "        model.addConstr(quicksum(W[i, j] for j in V2) == size)\n",
    "\n",
    "    for k in V4:\n",
    "        model.addConstr(item_group[k] == quicksum(W[i, j] * Ihelp[i][j][k] for i in V1 for j in V2))\n",
    "\n",
    "    for k in V4:\n",
    "        model.addConstr(abs_diff[k] >= item_group[k] - mean[k])\n",
    "        model.addConstr(abs_diff[k] >= -(item_group[k] - mean[k]))\n",
    "\n",
    "    model.addConstr(item_fair == quicksum(abs_diff[k] for k in V4))\n",
    "\n",
    "\n",
    "    # optimizing\n",
    "    model.optimize()\n",
    "    if model.status == GRB.OPTIMAL:\n",
    "        solution = model.getAttr('x', W)\n",
    "        #fairness = model.getAttr('x', item_group)\n",
    "\n",
    "\n",
    "    return solution\n",
    "\n",
    "def ideal(matrix, num_users, k):\n",
    "\n",
    "    group_count = [sum(row[i] for row in matrix) for i in range(len(matrix[0]))]\n",
    "    total = sum(group_count)\n",
    "    exposure = num_users * k\n",
    "    distribution = [x / total * exposure for x in group_count]\n",
    "\n",
    "    return distribution\n",
    "\n",
    "class RAIFPro(Abstract_Reranker):\n",
    "    def __init__(self, config, weights = None):\n",
    "        super().__init__(config, weights)\n",
    "\n",
    "\n",
    "    def rerank(self, ranking_score, k):\n",
    "        ## its parameters\n",
    "        topk = self.config['candidate']\n",
    "        alpha = self.config['alpha']\n",
    "\n",
    "        topk_items, topk_scores, num_users = load_ranking_matrices(ranking_score, topk)\n",
    "\n",
    "        #ideal exposure across groups\n",
    "        mean = ideal(self.M, num_users, k)\n",
    "\n",
    "        Ihelp = read_item_index(total_users=num_users, topk=topk, no_item_groups=self.group_num, item_group_map=self.iid2pid, topk_items=topk_items)\n",
    "        solution = fairness_optimisation(num_users, alpha, k, topk, self.group_num, Ihelp, topk_scores, mean)\n",
    "        rerank_list = get_results(num_users, k, topk, solution, topk_items)\n",
    "\n",
    "        return rerank_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geYqxGXkOsRF"
   },
   "source": [
    "**Step 3:** Create the configuration file under `FairDiverse-master/fairdiverse/recommendation/properties/models/` with the name of the model.yaml (e.g. RAIFPro.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "congif_raifpro = {\n",
    "    \"alpha\": 0.2,      #the weight parameter of item fairness term\n",
    "    \"candidate\": 100   #the number of item candidates\n",
    "}\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"recommendation/properties/models/RAIFPro.yaml\"\n",
    "\n",
    "\n",
    "# Write the dictionary to the YAML file\n",
    "with open(file_path, 'w') as file:\n",
    "    yaml.dump(congif_raifpro, file, default_flow_style=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Create a running configuration file `postprocessing_new_model.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"RAIFPro\"\n",
    "\n",
    "today = date.today()\n",
    "today_format = f\"{today.year}-{today.month}-{today.day}\"\n",
    "\n",
    "config_new_model = {\n",
    "    \"ranking_store_path\": f\"{today_format}_{base_model_name}_{dataset_name}\",  # Path to the ranking score file (required for post-processing)\n",
    "\n",
    "    # Change to any of the supported post-processing methods in Fairdiverse\n",
    "    \"model\": f\"{model_name}\",\n",
    "    \"fair-rank\": True,\n",
    "    \"log_name\": f\"{model_name}_{dataset_name}\", # path to save the evaluation\n",
    "\n",
    "    # Evaluation parameters\n",
    "    \"topk\": [5, 10, 20],\n",
    "    \"fairness_metrics\": [\"MinMaxRatio\", \"MMF\", \"GINI\", \"Entropy\"],\n",
    "    \"fairness_type\": \"Exposure\"  # \"Exposure\" computes exposure of item group; \"Utility\" computes score differences\n",
    "}\n",
    "\n",
    "with open(f\"recommendation/postprocessing_new_model.yaml\", \"w\") as file:\n",
    "    yaml.dump(config_new_model, file, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Import your custom model package in the corresponding file `fairdiverse/recommendation/rerank_model/__init__.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = \"recommendation/rerank_model/__init__.py\"\n",
    "\n",
    "# Read the current contents\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Define the new line to append\n",
    "new_import = \"\\nfrom .RAIFPro import RAIFPro\\n\"\n",
    "\n",
    "# Append only if it's not already present\n",
    "if new_import not in lines:\n",
    "    lines.append(new_import)\n",
    "\n",
    "# Write back to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6:** Define the model in the script `FairDiverse-master/fairdiverse/recommendation/reranker.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the target Python file\n",
    "file_path = \"recommendation/reranker.py\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Step 1: Update import line\n",
    "for i, line in enumerate(lines):\n",
    "    if \"from .rerank_model import\" in line:\n",
    "        if \"RAIFPro\" not in line:\n",
    "            lines[i] = line.strip() + \", RAIFPro\\n\"\n",
    "        break  # Only modify the first matching import line\n",
    "\n",
    "# Step 2: Add the RAIFPro elif clause\n",
    "new_elif_block = [\n",
    "    \"elif config['model'] == 'RAIFPro':\\n\",\n",
    "    \"    Reranker = RAIFPro(config)\\n\"\n",
    "]\n",
    "\n",
    "# Insert just before the existing \"else:\" clause inside the rerank method\n",
    "for i, line in enumerate(lines):\n",
    "    if \"else:\" in line and \"raise NotImplementedError\" in lines[i + 1]:\n",
    "        indent = \" \" * (len(line) - len(line.lstrip()))\n",
    "        # Make sure to adjust indentation to match\n",
    "        lines[i:i] = [indent + l for l in new_elif_block]\n",
    "        break\n",
    "\n",
    "# Save the modified file\n",
    "with open(file_path, 'w') as f:\n",
    "    f.writelines(lines)\n",
    "\n",
    "print(\"RAIFPro support added successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7:** To solve RAIFPro, we need install and set Gurobi.\n",
    "\n",
    "1. install packages in environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mip\n",
    "!pip install gurobipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. install Gurobi license \"Named-User Academic\" to your laptop\n",
    "https://portal.gurobi.com/iam/licenses/request\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grbgetkey xxxxxxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8:** Run RAIFPro for fairness-aware reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! python \"main.py\" --task recommendation --stage \"post-processing\" --dataset \"steam\" --train_config_file \"postprocessing_new_model.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Evaluation Resultsüìà**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation on fairness re-ranking algorithm applied on <base_model>\n",
    "print_evaluation_results(model_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation_results(base_model_name, dataset_name)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bsarec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
