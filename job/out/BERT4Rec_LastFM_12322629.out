============================================================================================== 
Warning! Mixing Conda and module environments may lead to corruption of the
user environment. 
We do not recommend users mixing those two environments unless absolutely
necessary. Note that 
SURF does not provide any support for Conda environment.
For more information, please refer to our software policy page:
https://servicedesk.surf.nl/wiki/display/WIKI/Software+policy+Snellius#SoftwarepolicySnellius-UseofAnacondaandMinicondaenvironmentsonSnellius 

Remember that many packages have already been installed on the system and can
be loaded using 
the 'module load <package__name>' command. If you are uncertain if a package is
already available 
on the system, please use 'module avail' or 'module spider' to search for it.
============================================================================================== 
usage: main.py [-h] [--data_dir DATA_DIR] [--output_dir OUTPUT_DIR]
               [--data_name DATA_NAME] [--do_eval] [--load_model LOAD_MODEL]
               [--train_name TRAIN_NAME] [--num_items NUM_ITEMS]
               [--num_users NUM_USERS] [--lr LR] [--batch_size BATCH_SIZE]
               [--epochs EPOCHS] [--no_cuda] [--log_freq LOG_FREQ]
               [--patience PATIENCE] [--num_workers NUM_WORKERS] [--seed SEED]
               [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]
               [--adam_beta2 ADAM_BETA2] [--gpu_id GPU_ID]
               [--variance VARIANCE] [--model_type MODEL_TYPE]
               [--max_seq_length MAX_SEQ_LENGTH] [--hidden_size HIDDEN_SIZE]
               [--num_hidden_layers NUM_HIDDEN_LAYERS]
               [--hidden_act HIDDEN_ACT]
               [--num_attention_heads NUM_ATTENTION_HEADS]
               [--attention_probs_dropout_prob ATTENTION_PROBS_DROPOUT_PROB]
               [--hidden_dropout_prob HIDDEN_DROPOUT_PROB]
               [--initializer_range INITIALIZER_RANGE] [--c C] [--alpha ALPHA]
main.py: error: unrecognized arguments: --mask_ratio 0.4
2025-06-12 21:49:23,555 - Namespace(data_dir='data/', output_dir='output/', data_name='LastFM', do_eval=True, load_model='BERT4Rec_LastFM', train_name='Jun-12-2025_21-49-23', num_items=10, num_users=1091, lr=0.001, batch_size=256, epochs=200, no_cuda=False, log_freq=1, patience=10, num_workers=4, seed=42, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, gpu_id='0', variance=5, model_type='BSARec', max_seq_length=50, hidden_size=64, num_hidden_layers=2, hidden_act='gelu', num_attention_heads=2, attention_probs_dropout_prob=0.5, hidden_dropout_prob=0.5, initializer_range=0.02, c=3, alpha=0.9, cuda_condition=True, data_file='data/LastFM.txt', item_size=3647, checkpoint_path='output/Jun-12-2025_21-49-23.pt', same_target_path='data/LastFM_same_target.npy')
2025-06-12 21:49:23,568 - BSARecModel(
  (item_embeddings): Embedding(3647, 64, padding_idx=0)
  (position_embeddings): Embedding(50, 64)
  (LayerNorm): LayerNorm()
  (dropout): Dropout(p=0.5, inplace=False)
  (item_encoder): BSARecEncoder(
    (blocks): ModuleList(
      (0): BSARecBlock(
        (layer): BSARecLayer(
          (filter_layer): FrequencyLayer(
            (out_dropout): Dropout(p=0.5, inplace=False)
            (LayerNorm): LayerNorm()
          )
          (attention_layer): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm()
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (1): BSARecBlock(
        (layer): BSARecLayer(
          (filter_layer): FrequencyLayer(
            (out_dropout): Dropout(p=0.5, inplace=False)
            (LayerNorm): LayerNorm()
          )
          (attention_layer): MultiHeadAttention(
            (query): Linear(in_features=64, out_features=64, bias=True)
            (key): Linear(in_features=64, out_features=64, bias=True)
            (value): Linear(in_features=64, out_features=64, bias=True)
            (softmax): Softmax(dim=-1)
            (attn_dropout): Dropout(p=0.5, inplace=False)
            (dense): Linear(in_features=64, out_features=64, bias=True)
            (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
            (out_dropout): Dropout(p=0.5, inplace=False)
          )
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm()
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
)
2025-06-12 21:49:25,728 - Total Parameters: 337088
2025-06-12 21:49:25,765 - odict_keys(['item_embeddings.weight', 'position_embeddings.weight', 'LayerNorm.weight', 'LayerNorm.bias', 'item_encoder.blocks.0.layer.filter_layer.sqrt_beta', 'item_encoder.blocks.0.layer.filter_layer.LayerNorm.weight', 'item_encoder.blocks.0.layer.filter_layer.LayerNorm.bias', 'item_encoder.blocks.0.layer.attention_layer.query.weight', 'item_encoder.blocks.0.layer.attention_layer.query.bias', 'item_encoder.blocks.0.layer.attention_layer.key.weight', 'item_encoder.blocks.0.layer.attention_layer.key.bias', 'item_encoder.blocks.0.layer.attention_layer.value.weight', 'item_encoder.blocks.0.layer.attention_layer.value.bias', 'item_encoder.blocks.0.layer.attention_layer.dense.weight', 'item_encoder.blocks.0.layer.attention_layer.dense.bias', 'item_encoder.blocks.0.layer.attention_layer.LayerNorm.weight', 'item_encoder.blocks.0.layer.attention_layer.LayerNorm.bias', 'item_encoder.blocks.0.feed_forward.dense_1.weight', 'item_encoder.blocks.0.feed_forward.dense_1.bias', 'item_encoder.blocks.0.feed_forward.dense_2.weight', 'item_encoder.blocks.0.feed_forward.dense_2.bias', 'item_encoder.blocks.0.feed_forward.LayerNorm.weight', 'item_encoder.blocks.0.feed_forward.LayerNorm.bias', 'item_encoder.blocks.1.layer.filter_layer.sqrt_beta', 'item_encoder.blocks.1.layer.filter_layer.LayerNorm.weight', 'item_encoder.blocks.1.layer.filter_layer.LayerNorm.bias', 'item_encoder.blocks.1.layer.attention_layer.query.weight', 'item_encoder.blocks.1.layer.attention_layer.query.bias', 'item_encoder.blocks.1.layer.attention_layer.key.weight', 'item_encoder.blocks.1.layer.attention_layer.key.bias', 'item_encoder.blocks.1.layer.attention_layer.value.weight', 'item_encoder.blocks.1.layer.attention_layer.value.bias', 'item_encoder.blocks.1.layer.attention_layer.dense.weight', 'item_encoder.blocks.1.layer.attention_layer.dense.bias', 'item_encoder.blocks.1.layer.attention_layer.LayerNorm.weight', 'item_encoder.blocks.1.layer.attention_layer.LayerNorm.bias', 'item_encoder.blocks.1.feed_forward.dense_1.weight', 'item_encoder.blocks.1.feed_forward.dense_1.bias', 'item_encoder.blocks.1.feed_forward.dense_2.weight', 'item_encoder.blocks.1.feed_forward.dense_2.bias', 'item_encoder.blocks.1.feed_forward.LayerNorm.weight', 'item_encoder.blocks.1.feed_forward.LayerNorm.bias'])
Traceback (most recent call last):
  File "/gpfs/home3/scur2771/BSARec/BSARec/src/main.py", line 100, in <module>
    main()
  File "/gpfs/home3/scur2771/BSARec/BSARec/src/main.py", line 56, in main
    trainer.load(args.checkpoint_path)
  File "/gpfs/home3/scur2771/BSARec/BSARec/src/trainers.py", line 50, in load
    new_dict = torch.load(file_name)
  File "/home/scur2771/.conda/envs/bsarec/lib/python3.9/site-packages/torch/serialization.py", line 579, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/scur2771/.conda/envs/bsarec/lib/python3.9/site-packages/torch/serialization.py", line 230, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/scur2771/.conda/envs/bsarec/lib/python3.9/site-packages/torch/serialization.py", line 211, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'output/BERT4Rec_LastFM.pt'

JOB STATISTICS
==============
Job ID: 12322629
Cluster: snellius
User/Group: scur2771/scur2771
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 00:04:48 core-walltime
Job Wall-clock time: 00:00:16
Memory Utilized: 0.00 MB
Memory Efficiency: 0.00% of 120.00 GB (120.00 GB/node)
WARNING: Efficiency statistics can only be obtained after the job has ended as seff tool is based on the accounting database data.
