2025-06-14 21:39:53,944 - Namespace(data_dir='data/self_processed/', output_dir='output/', data_name='Diginetica', do_eval=True, load_model='SASRec_Diginetica', train_name='Jun-14-2025_21-39-52', num_items=10, num_users=14829, lr=0.001, batch_size=256, epochs=200, no_cuda=False, log_freq=1, patience=10, num_workers=4, seed=42, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, gpu_id='0', variance=5, model_type='SASRec', max_seq_length=50, hidden_size=64, num_hidden_layers=2, hidden_act='gelu', num_attention_heads=1, attention_probs_dropout_prob=0.5, hidden_dropout_prob=0.5, initializer_range=0.02, cuda_condition=True, data_file='data/self_processed/Diginetica.txt', item_size=9441, checkpoint_path='output/Jun-14-2025_21-39-52.pt', same_target_path='data/self_processed/Diginetica_same_target.npy')
2025-06-14 21:39:53,962 - SASRecModel(
  (item_embeddings): Embedding(9441, 64, padding_idx=0)
  (position_embeddings): Embedding(50, 64)
  (LayerNorm): LayerNorm()
  (dropout): Dropout(p=0.5, inplace=False)
  (item_encoder): TransformerEncoder(
    (blocks): ModuleList(
      (0): TransformerBlock(
        (layer): MultiHeadAttention(
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (softmax): Softmax(dim=-1)
          (attn_dropout): Dropout(p=0.5, inplace=False)
          (dense): Linear(in_features=64, out_features=64, bias=True)
          (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm()
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (1): TransformerBlock(
        (layer): MultiHeadAttention(
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (softmax): Softmax(dim=-1)
          (attn_dropout): Dropout(p=0.5, inplace=False)
          (dense): Linear(in_features=64, out_features=64, bias=True)
          (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm()
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
)
2025-06-14 21:39:56,148 - Total Parameters: 707520
2025-06-14 21:39:56,219 - odict_keys(['item_embeddings.weight', 'position_embeddings.weight', 'LayerNorm.weight', 'LayerNorm.bias', 'item_encoder.blocks.0.layer.query.weight', 'item_encoder.blocks.0.layer.query.bias', 'item_encoder.blocks.0.layer.key.weight', 'item_encoder.blocks.0.layer.key.bias', 'item_encoder.blocks.0.layer.value.weight', 'item_encoder.blocks.0.layer.value.bias', 'item_encoder.blocks.0.layer.dense.weight', 'item_encoder.blocks.0.layer.dense.bias', 'item_encoder.blocks.0.layer.LayerNorm.weight', 'item_encoder.blocks.0.layer.LayerNorm.bias', 'item_encoder.blocks.0.feed_forward.dense_1.weight', 'item_encoder.blocks.0.feed_forward.dense_1.bias', 'item_encoder.blocks.0.feed_forward.dense_2.weight', 'item_encoder.blocks.0.feed_forward.dense_2.bias', 'item_encoder.blocks.0.feed_forward.LayerNorm.weight', 'item_encoder.blocks.0.feed_forward.LayerNorm.bias', 'item_encoder.blocks.1.layer.query.weight', 'item_encoder.blocks.1.layer.query.bias', 'item_encoder.blocks.1.layer.key.weight', 'item_encoder.blocks.1.layer.key.bias', 'item_encoder.blocks.1.layer.value.weight', 'item_encoder.blocks.1.layer.value.bias', 'item_encoder.blocks.1.layer.dense.weight', 'item_encoder.blocks.1.layer.dense.bias', 'item_encoder.blocks.1.layer.LayerNorm.weight', 'item_encoder.blocks.1.layer.LayerNorm.bias', 'item_encoder.blocks.1.feed_forward.dense_1.weight', 'item_encoder.blocks.1.feed_forward.dense_1.bias', 'item_encoder.blocks.1.feed_forward.dense_2.weight', 'item_encoder.blocks.1.feed_forward.dense_2.bias', 'item_encoder.blocks.1.feed_forward.LayerNorm.weight', 'item_encoder.blocks.1.feed_forward.LayerNorm.bias'])
2025-06-14 21:39:56,227 - odict_keys(['item_embeddings.weight', 'position_embeddings.weight', 'LayerNorm.weight', 'LayerNorm.bias', 'item_encoder.blocks.0.layer.query.weight', 'item_encoder.blocks.0.layer.query.bias', 'item_encoder.blocks.0.layer.key.weight', 'item_encoder.blocks.0.layer.key.bias', 'item_encoder.blocks.0.layer.value.weight', 'item_encoder.blocks.0.layer.value.bias', 'item_encoder.blocks.0.layer.dense.weight', 'item_encoder.blocks.0.layer.dense.bias', 'item_encoder.blocks.0.layer.LayerNorm.weight', 'item_encoder.blocks.0.layer.LayerNorm.bias', 'item_encoder.blocks.0.feed_forward.dense_1.weight', 'item_encoder.blocks.0.feed_forward.dense_1.bias', 'item_encoder.blocks.0.feed_forward.dense_2.weight', 'item_encoder.blocks.0.feed_forward.dense_2.bias', 'item_encoder.blocks.0.feed_forward.LayerNorm.weight', 'item_encoder.blocks.0.feed_forward.LayerNorm.bias', 'item_encoder.blocks.1.layer.query.weight', 'item_encoder.blocks.1.layer.query.bias', 'item_encoder.blocks.1.layer.key.weight', 'item_encoder.blocks.1.layer.key.bias', 'item_encoder.blocks.1.layer.value.weight', 'item_encoder.blocks.1.layer.value.bias', 'item_encoder.blocks.1.layer.dense.weight', 'item_encoder.blocks.1.layer.dense.bias', 'item_encoder.blocks.1.layer.LayerNorm.weight', 'item_encoder.blocks.1.layer.LayerNorm.bias', 'item_encoder.blocks.1.feed_forward.dense_1.weight', 'item_encoder.blocks.1.feed_forward.dense_1.bias', 'item_encoder.blocks.1.feed_forward.dense_2.weight', 'item_encoder.blocks.1.feed_forward.dense_2.bias', 'item_encoder.blocks.1.feed_forward.LayerNorm.weight', 'item_encoder.blocks.1.feed_forward.LayerNorm.bias'])
2025-06-14 21:39:56,228 - Load model from output/SASRec_Diginetica.pt for test!
2025-06-14 21:40:00,172 - {'Epoch': 0, 'HR@5': '0.1035', 'NDCG@5': '0.0667', 'HR@10': '0.1681', 'NDCG@10': '0.0875', 'HR@20': '0.2501', 'NDCG@20': '0.1081'}
2025-06-14 21:40:00,439 - Saved predictions in `output/predictions/SASRec_Diginetica_predictions.csv`
2025-06-14 21:40:00,439 - Jun-14-2025_21-39-52
2025-06-14 21:40:00,439 - {'Epoch': 0, 'HR@5': '0.1035', 'NDCG@5': '0.0667', 'HR@10': '0.1681', 'NDCG@10': '0.0875', 'HR@20': '0.2501', 'NDCG@20': '0.1081'}
2025-06-23 12:42:51,910 - Namespace(data_dir='data/self_processed/', output_dir='output/', data_name='Diginetica', do_val_eval=False, do_eval=True, load_model='SASRec_Diginetica', train_name='Jun-23-2025_12-42-50', num_items=10, num_users=14829, lr=0.001, batch_size=256, epochs=200, no_cuda=False, log_freq=1, patience=10, num_workers=4, seed=42, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, gpu_id='0', variance=5, model_type='SASRec', max_seq_length=50, hidden_size=64, num_hidden_layers=2, hidden_act='gelu', num_attention_heads=1, attention_probs_dropout_prob=0.5, hidden_dropout_prob=0.5, initializer_range=0.02, cuda_condition=True, data_file='data/self_processed/Diginetica.txt', item_size=9441, checkpoint_path='output/Jun-23-2025_12-42-50.pt', same_target_path='data/self_processed/Diginetica_same_target.npy')
2025-06-23 12:42:51,928 - SASRecModel(
  (item_embeddings): Embedding(9441, 64, padding_idx=0)
  (position_embeddings): Embedding(50, 64)
  (LayerNorm): LayerNorm()
  (dropout): Dropout(p=0.5, inplace=False)
  (item_encoder): TransformerEncoder(
    (blocks): ModuleList(
      (0): TransformerBlock(
        (layer): MultiHeadAttention(
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (softmax): Softmax(dim=-1)
          (attn_dropout): Dropout(p=0.5, inplace=False)
          (dense): Linear(in_features=64, out_features=64, bias=True)
          (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm()
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (1): TransformerBlock(
        (layer): MultiHeadAttention(
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (softmax): Softmax(dim=-1)
          (attn_dropout): Dropout(p=0.5, inplace=False)
          (dense): Linear(in_features=64, out_features=64, bias=True)
          (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm()
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
)
2025-06-23 12:42:54,072 - Total Parameters: 707520
2025-06-23 12:42:54,143 - odict_keys(['item_embeddings.weight', 'position_embeddings.weight', 'LayerNorm.weight', 'LayerNorm.bias', 'item_encoder.blocks.0.layer.query.weight', 'item_encoder.blocks.0.layer.query.bias', 'item_encoder.blocks.0.layer.key.weight', 'item_encoder.blocks.0.layer.key.bias', 'item_encoder.blocks.0.layer.value.weight', 'item_encoder.blocks.0.layer.value.bias', 'item_encoder.blocks.0.layer.dense.weight', 'item_encoder.blocks.0.layer.dense.bias', 'item_encoder.blocks.0.layer.LayerNorm.weight', 'item_encoder.blocks.0.layer.LayerNorm.bias', 'item_encoder.blocks.0.feed_forward.dense_1.weight', 'item_encoder.blocks.0.feed_forward.dense_1.bias', 'item_encoder.blocks.0.feed_forward.dense_2.weight', 'item_encoder.blocks.0.feed_forward.dense_2.bias', 'item_encoder.blocks.0.feed_forward.LayerNorm.weight', 'item_encoder.blocks.0.feed_forward.LayerNorm.bias', 'item_encoder.blocks.1.layer.query.weight', 'item_encoder.blocks.1.layer.query.bias', 'item_encoder.blocks.1.layer.key.weight', 'item_encoder.blocks.1.layer.key.bias', 'item_encoder.blocks.1.layer.value.weight', 'item_encoder.blocks.1.layer.value.bias', 'item_encoder.blocks.1.layer.dense.weight', 'item_encoder.blocks.1.layer.dense.bias', 'item_encoder.blocks.1.layer.LayerNorm.weight', 'item_encoder.blocks.1.layer.LayerNorm.bias', 'item_encoder.blocks.1.feed_forward.dense_1.weight', 'item_encoder.blocks.1.feed_forward.dense_1.bias', 'item_encoder.blocks.1.feed_forward.dense_2.weight', 'item_encoder.blocks.1.feed_forward.dense_2.bias', 'item_encoder.blocks.1.feed_forward.LayerNorm.weight', 'item_encoder.blocks.1.feed_forward.LayerNorm.bias'])
2025-06-23 12:42:54,150 - odict_keys(['item_embeddings.weight', 'position_embeddings.weight', 'LayerNorm.weight', 'LayerNorm.bias', 'item_encoder.blocks.0.layer.query.weight', 'item_encoder.blocks.0.layer.query.bias', 'item_encoder.blocks.0.layer.key.weight', 'item_encoder.blocks.0.layer.key.bias', 'item_encoder.blocks.0.layer.value.weight', 'item_encoder.blocks.0.layer.value.bias', 'item_encoder.blocks.0.layer.dense.weight', 'item_encoder.blocks.0.layer.dense.bias', 'item_encoder.blocks.0.layer.LayerNorm.weight', 'item_encoder.blocks.0.layer.LayerNorm.bias', 'item_encoder.blocks.0.feed_forward.dense_1.weight', 'item_encoder.blocks.0.feed_forward.dense_1.bias', 'item_encoder.blocks.0.feed_forward.dense_2.weight', 'item_encoder.blocks.0.feed_forward.dense_2.bias', 'item_encoder.blocks.0.feed_forward.LayerNorm.weight', 'item_encoder.blocks.0.feed_forward.LayerNorm.bias', 'item_encoder.blocks.1.layer.query.weight', 'item_encoder.blocks.1.layer.query.bias', 'item_encoder.blocks.1.layer.key.weight', 'item_encoder.blocks.1.layer.key.bias', 'item_encoder.blocks.1.layer.value.weight', 'item_encoder.blocks.1.layer.value.bias', 'item_encoder.blocks.1.layer.dense.weight', 'item_encoder.blocks.1.layer.dense.bias', 'item_encoder.blocks.1.layer.LayerNorm.weight', 'item_encoder.blocks.1.layer.LayerNorm.bias', 'item_encoder.blocks.1.feed_forward.dense_1.weight', 'item_encoder.blocks.1.feed_forward.dense_1.bias', 'item_encoder.blocks.1.feed_forward.dense_2.weight', 'item_encoder.blocks.1.feed_forward.dense_2.bias', 'item_encoder.blocks.1.feed_forward.LayerNorm.weight', 'item_encoder.blocks.1.feed_forward.LayerNorm.bias'])
2025-06-23 12:42:54,151 - Load model from output/SASRec_Diginetica.pt for test!
2025-06-23 12:42:57,894 - {'Epoch': 0, 'HR@5': '0.1035', 'NDCG@5': '0.0665', 'HR@10': '0.1639', 'NDCG@10': '0.0859', 'HR@20': '0.2503', 'NDCG@20': '0.1076'}
2025-06-23 12:42:58,169 - Saved predictions in `output/predictions/SASRec_Diginetica_predictions.csv`
2025-06-23 12:42:58,169 - Jun-23-2025_12-42-50
2025-06-23 12:42:58,169 - {'Epoch': 0, 'HR@5': '0.1035', 'NDCG@5': '0.0665', 'HR@10': '0.1639', 'NDCG@10': '0.0859', 'HR@20': '0.2503', 'NDCG@20': '0.1076'}
